{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreword\n",
    "\n",
    "The most important requirement of a code is not its switftness. It is that it provides the right answer. Then, and only then is it time to optimize if its performance proves unsatisfactory for your needs. The strongest gains are often to be found not in the few tips and tricks presented here but in the structure of the code and its high level algorithms, points that are well beyond the scope of this short tutorial. It is therefore important to consider such aspects before trying to optimize the details of the implementation. Yet, as we will see, these can nevertheless yield nice speedups for a minimal investment.\n",
    "\n",
    "These tips and tricks mostly come from my experience developing [Cigale](http:://cigale.lam.fr). I do not claim they are original (they certainly are not) and I would be more than happy to add new ideas to this notebook. Come talk to me ot send me a word at mederic.boquien@uantof.cl.\n",
    "\n",
    "Note: you will likely need a recent version of Python 3 to run all of the examples below.\n",
    "\n",
    "---\n",
    "\n",
    "# A couple of useful tools\n",
    "\n",
    "Two of the main traps when attempting to optimize a program are: 1. to assume you know what the bottlenecks of your code are, and 2. to assume that a method is faster than another. Do **not** assume. It is not unlikely that you are incorrect in your assumptions. Profile, *profile*, **profile**. There are numerous great tools around to do just that. I especially recommend a couple of them that have proven tremendously useful for me: [gprof2dot](https://github.com/jrfonseca/gprof2dot) and [line_profiler](https://github.com/rkern/line_profiler). A small limitation though: these tools do not work properly when running a parallel code.\n",
    "\n",
    "## gprof2dot\n",
    "\n",
    "This is a script that creates call graphs, shows what are the hot paths in your program, and how much time is spent in the most time-consuming functions so you can identify the bottlenecks and know where to focus your efforts. Its usage is simple:\n",
    "```bash\n",
    "python -m profile -o output.pstats path/to/your/script.py arg1 arg2\n",
    "gprof2dot -f pstats output.pstats | dot -Tsvg -o output.svg\n",
    "```\n",
    "This generates an SVG file of your call graph, which can be displayed, for instance, with Chrome or Firefox. Bonus, function names are searchable as in a web page. Note: if you cannot find the dot program, it comes with [Graphviz](http://www.graphviz.org/).\n",
    "\n",
    "## line\\_profiler\n",
    "\n",
    "This is a script that display how much time is spent on each instruction in specific functions. To use it simply add the @profile decorator to the functions you want to profile and run your script as follows:\n",
    "```bash\n",
    "kernprof -l path/to/your/script arg1 arg2\n",
    "python -m line_profiler script.py.lprof\n",
    "```\n",
    "This will display the time spent in each function and for each line, the number of hits, the time spent per hit, the percentage of time spent on a line compared to the total time, and the content of the line.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Avoid loops\n",
    "\n",
    "Let's start by stating the obvious: python loops are slow and should be avoided as much as possible. As an example, let's compute the sum of the *n* first natural integers first with a pure python loop, then without a loop but still in pure python, and finally using Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loop_python(n):\n",
    "    intsum = 0\n",
    "    for i in range(1, n+1):\n",
    "        intsum += n\n",
    "    return intsum\n",
    "\n",
    "def noloop_python(n):\n",
    "    return sum(range(1, n+1))\n",
    "\n",
    "def noloop_numpy(n):\n",
    "    return np.sum(np.arange(1, n+1))\n",
    "\n",
    "n = 1000000\n",
    "%timeit loop_python(n)\n",
    "%timeit noloop_python(n)\n",
    "%timeit noloop_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes loops are hard to avoid. In that case it is better to have few iterations on large arrays rather than many iterations on small arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_sum(arr):\n",
    "    arrsum = np.zeros(arr.shape[1])\n",
    "    for i in range(arr.shape[0]):\n",
    "        arrsum += np.sum(arr[i, :])\n",
    "    return np.sum(arrsum)\n",
    "\n",
    "x = np.random.rand(1000000).reshape(10, 100000)\n",
    "y = np.random.rand(1000000).reshape(100000, 10)\n",
    "\n",
    "%timeit compute_sum(x)\n",
    "%timeit compute_sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: avoid loops whenever possible and when it is not possible ensure that each iteration operates on as many elements as possible.**\n",
    "\n",
    "---\n",
    "\n",
    "# Stay in place\n",
    "\n",
    "Two types of operations are possible on an array: in-place or not in-place. When an operation is in-place it means the operation is carried out directly in the array. Otherwise it goes through intermediate arrays whose allocation can be costly and kill the cache of the CPU.\n",
    "\n",
    "You may be familiar with the in-place operations as they come with handy shorthands: +=, -=, \\*=, /=, or \\*\\*= for the most common ones. As they avoid the aforementioned array allocation, they happen to be quite a bit faster than the alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inplace_sum(arr):\n",
    "    arr += 0.\n",
    "\n",
    "def notinplace_sum(arr):\n",
    "    arr = arr + 0.\n",
    "\n",
    "%timeit inplace_sum(x)\n",
    "%timeit notinplace_sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for complex operations, it may be worthwhile to decompose them in terms of elementary in-place operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inplace_norm(arr):\n",
    "    vmax = np.max(arr)\n",
    "    vmin = np.min(arr)\n",
    "    arr -= vmax\n",
    "    arr *= 1. / (vmax - vmin)\n",
    "\n",
    "def notinplace_norm(arr):\n",
    "    vmax = np.max(arr)\n",
    "    vmin = np.min(arr)\n",
    "    arr = (arr - vmax) * (1. / (vmax - vmin))\n",
    "\n",
    "%timeit inplace_norm(x)\n",
    "%timeit notinplace_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: avoid complex formulas that require the creation of temporary arrays. Series of in-place operations are likely faster (though a bit less readable sometimes).**\n",
    "\n",
    "---\n",
    "\n",
    "# Do not get fancy unless needed\n",
    "\n",
    "Fancy indexing is really a nice and convenient way to make complex selections on arrays. However fancy indexing can be quite slow and an array indexed that way generates a copy. When possible, it is better to use slices as they only create a view of the array with just the right elements. Let's compare the two approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(10000000)\n",
    "z = np.tile(np.arange(10.), 1000000)\n",
    "\n",
    "w = np.where(z == 3.)\n",
    "\n",
    "print('Are the two arrays the same? {}'.format(np.all(x[w] == x[3::10])))\n",
    "print('Do the two arrays share memory? {}'.format(np.may_share_memory(x[w], x[3::10])))\n",
    "\n",
    "%timeit x[w]\n",
    "%timeit x[3::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sometimes it is convenient to create slices programmatically. This can be done easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl = slice(3, None, 10)\n",
    "\n",
    "print('Are the two arrays the same? {}'.format(np.all(x[sl] == x[3::10])))\n",
    "print('Do the two arrays share memory? {}'.format(np.may_share_memory(x[sl], x[3::10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: it is faster and saves memory to use slices. Beware if you modify the resulting array though.**\n",
    "\n",
    "---\n",
    "\n",
    "# <s>Divide</s>Multiply and conquer\n",
    "Modern CPU have become good at handling floats natively with fairly good efficiency. However not all basic operations on floats have been created equal and some are faster than others. Let's find out which."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(1000000)\n",
    "\n",
    "%timeit x + np.pi\n",
    "%timeit x - np.pi\n",
    "%timeit x * np.pi\n",
    "%timeit x / np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: avoid dividing arrays, prefer multiplications when possible. Note that CPUs are good at dividing by (powers of) 2. In that case the speed difference is much smaller.**\n",
    "\n",
    "---\n",
    "\n",
    "# Memory matters\n",
    "How we store multi-dimensional arrays can have an important effect on the final performance. Let's start with a small experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(5000**2).reshape(5000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.sum(x, axis=0)\n",
    "%timeit np.sum(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operating on the second axis appears faster than operating on the first axis. Each time we operate on the same number of elements so it does not relate to the size of the array. This is actually due to how the array is stored in memory. Numpy arrays are C-ordered. This means that the last dimension is the one that varies the fastest. In effect x is stored linearly in memory as follows: 1 2 … 4998 4999 5000 5001 … 9998 9999 10000 10001 … […] … 24999998 24999999. So when we operate on the second axis, numpy operates on contiguous arrays: 1 2 … 4998 4999, then 5000 5001 … 9998 9999, etc. CPU are very good at reading contiguous data as they can read the data in advance (prefetch) and can cache them efficiently. When we operate on the first axis, things go very differently though. Numpy operates on non-contiguous arrays: 0 5000 … 24990000 24995000 then 1 5001 … 24990001 24995001. As these values are stored in very different locations in memory, the CPU cannot read the data ahead of being required (note that it can if the stride is not too large but here the stride is ~40kB, which is too large even for modern CPUs).\n",
    "\n",
    "**Conclusion: whenever you can, have arrays shapes that ensure that you operate on the last axis as it is the fastest-varying index.**\n",
    "\n",
    "---\n",
    "\n",
    "# Cache\n",
    "\n",
    "The best optimization you can do when computing a quantity is not having to compute it at all. This is especially true if you often call a given function with the same arguments. In that case it would be smart to just keep the results in memory. This can be done using the @lru_cache decorator provided in the functools module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "\n",
    "def nocache_sum(n):\n",
    "    return np.sum(np.arange(1, n+1))\n",
    "\n",
    "@lru_cache()\n",
    "def cache_sum(n):\n",
    "    return np.sum(np.arange(1, n+1))\n",
    "\n",
    "n = 1000000\n",
    "%timeit nocache_sum(n)\n",
    "%timeit cache_sum(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is convenient, it does not cover the case where the parameters are not hashable, such as Numpy arrays. Under these circumstances the cache has to be done manually. A very naïve (and quite inefficient but that is just for the sake of illustration) cache implementation for a Numpy array would be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cache(func):\n",
    "    cache = {}\n",
    "    def helper(arr):\n",
    "        key = hash(arr.tobytes()) # This is slow. It is only worthwhile if the cached function is very slow \n",
    "        if key not in cache:\n",
    "            cache[key] = func(arr)\n",
    "        return cache[key]\n",
    "    return helper\n",
    "\n",
    "def nocache_sumexp(arr):\n",
    "    return np.sum(np.exp(arr))\n",
    "    \n",
    "@cache\n",
    "def cache_sumexp(arr):\n",
    "    return np.sum(np.exp(arr))\n",
    "\n",
    "x = np.random.rand(1000000)\n",
    "%timeit nocache_sumexp(x)\n",
    "%timeit cache_sumexp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: caching frequently called and/or expensive functions can save you a tremendous amount of time at the cost of some RAM. The balance is yours to strike.**\n",
    "\n",
    "---\n",
    "\n",
    "# Eliminate Numpy overheads\n",
    "\n",
    "Maybe you are spending a large part of your time repeatedly calling the same Numpy function. Numpy tends to be quite careful when processing the arguments, making sure everything is alright, checking various corner cases, etc. This is a good thing and we want Numpy to be cautious. However in a very well controled environment where we are always in the same specific conditions, the overhead coming from numpy checks may come at such a cost that they want to be avoided. Let's take the Numpy trapezoidal rule integration function for instance. A large majority of the lines of code is taken by these checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.source(np.trapz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we are in the simple case of 1D integration and we are certain we pass 1D Numpy arrays. The implementation could be simplified to eliminate the overhead, which would yield a nice speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trapz_v1(y, x):\n",
    "    return (np.diff(x) * (y[1:]+y[:-1])).sum() / 2.\n",
    "\n",
    "x = np.logspace(1, 2, 1000)\n",
    "y = np.random.rand(1000)\n",
    "%timeit np.trapz(y, x)\n",
    "%timeit trapz_v1(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: the overhead of Numpy functions can be largely eliminated when the call conditions (type, dimension, etc.) are perfectly under control.**\n",
    "\n",
    "---\n",
    "\n",
    "# Going beyond\n",
    "\n",
    "Naturally these various tips and tricks do not come in isolation and can easily be combined. If we continue expanding from the trapz example, considering a case where we call it millions or billions of times, assuming the x grid is the same, it would be interesting to maybe cache dx using the naïve cache implementation earlier. However the computation of the hash is rapidly a bottleneck so a smarter algorithm would be needed. In any case, we saw that a multiplication is faster than a division so we can gain a few cycles at no cost multiplying by 0.5 rather than by dividing by 2. But if we pay close attention, the computation of the integral can be rewritten as a dot product. Dot products can be very fast with Numpy as they can use highly optimized linear algebra libraries such as [OpenBLAS](http://www.openblas.net/) for instance. They can be multi-threaded, wich gives a nice bonus when your program is not already parallel. To check whether you have optimized libraries you can simply look at the configuration of Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.__config__.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If neither OpenBLAS, ATLAS, nor MKL are available, it is likely that you only have the slower unoptimized library installed. In any case, even then using the dot product yields nice improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trapz_v2(y, x):\n",
    "    return np.dot(np.diff(x), y[1:]+y[:-1]) * .5\n",
    "\n",
    "x = np.logspace(1, 2, 1000)\n",
    "y = np.random.rand(1000)\n",
    "%timeit np.trapz(y, x)\n",
    "%timeit trapz_v1(y, x)\n",
    "%timeit trapz_v2(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: with a focused effort on the hotspots, code can be made much faster with just a few tips. With habit some of these tips can become second nature and help write naturally efficient code.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
